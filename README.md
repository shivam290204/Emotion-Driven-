üé≠ Emotion-Driven Interactive Storyteller AI-powered Python application that uses real-time facial emotion recognition to generate and narrate personalized short stories tailored to your mood. <img width="935" height="730" alt="Screenshot 2025-08-21 095705" src="https://github.com/user-attachments/assets/d1f457c9-f8cb-49de-a009-6e6546b5b1d2" />
<img width="981" height="703" alt="Screenshot 2025-08-21 095202" src="https://github.com/user-attachments/assets/6d50d194-0cbf-49ed-acdd-4b683f6115ab" />
This project explores the intersection of computer vision, artificial intelligence, and human-computer interaction. By leveraging a live webcam feed, the application analyzes the user's facial expression to detect their dominant emotion (e.g., happy, sad, surprised). Based on this detection, it crafts a unique short story and uses a text-to-speech engine to narrate it, creating a personalized and interactive experience. The entire user interface is built as a clean, modern web application using Streamlit, with the camera feed integrated directly into the page for a seamless experience.‚ú® FeaturesReal-Time Emotion Detection: Uses the DeepFace library to accurately identify emotions from a live video stream.Integrated Camera Feed: The webcam view is embedded directly into the web application using streamlit-webrtc, eliminating the need for separate pop-up windows.Personalized Story Generation: Dynamically selects or generates a story from a predefined collection that matches the user's detected emotion.Text-to-Speech Narration: Utilizes pyttsx3 to provide voice narration for the generated stories, working completely offline.Interactive UI: A clean and professional user interface built with Streamlit, featuring user settings, a live camera view, and a story display panel.Optimized Performance: The AI model is cached on startup to ensure fast performance during subsequent emotion scans.‚öôÔ∏è How It Works (Architecture)The application is built with a modular structure to ensure the code is clean, maintainable, and easy to understand.app.py (Main Application): This is the core file that runs the Streamlit web application. It handles the overall UI layout, user interactions (like button clicks), and orchestrates the communication between the other modules. It uses streamlit-webrtc to manage and display the live camemotion_detector.pyr.py (The Vision Engine): This module contains all the computer vision logic. It uses OpenCV to process video frames and DeepFace (with a TensorFlow backend) to perform the actual emotion analysis on each frame. The AI model is loaded and cached using Streamlit's caching mechanism for optimal performance.story_generator.py (The Content Engine): This module is responsible for all content-related tasks.It selects a story from story_data.py based on the detected emotion. It uses the pyttsx3 library to convert the story text into speech.story_data.py (The Database): Acts as a simple database containing the pre-written story templates, categorized by emotion.üõ†Ô∏è Technology StackLanguage: PythonWeb Framework: StreamlitComputer Vision: OpenCV, DeepFaceMachine Learning Backend: TensorFlowLive Video Streaming: streamlit-webrtcText-to-Speech: pyttsx3Data Handling: Pandas, Plotly (for charts)üöÄ How to Run This Project LocallyFollow these steps to set up and run the project on your local machine. 1. Clone the RepositoryFirst, clone this repository to your local machine. git clone <your-repository-url>
cd Emotion_storyTeller
2. Create a Virtual Environment is highly recommended to use a virtual environment to manage project dependencies.# Create the virtual environment
python -m venv venv

# Activate the virtual environment
# On Windows:
.\venv\Scripts\activate
# On macOS/Linux:
Source venv/bin/activate
3. Install Dependencies The required libraries are listed in the requirements.txt file. Install them using pip. pip install -r requirements.txt
4. Run theApplicatione the dependencies are installed, you can run the Streamlit application with a single command. python -m streamlit run app.py
The application should now open in a new tab in your web browser!üìñ Usage Once the application is running, you will see the main interface. Optionally, use the sidebar to enter your name for a more personalized story. Click the "Start Scan" button. Your browser will ask for permission to use your camera. Please allow it. The live camera feed will appear on the page. Position your face clearly in the view. The application will analyze your expression in real-time. Once it detects an emotion with sufficient confidence, the video will stop automatically. Your detected emotion and a personalized story will appear on the right-hand side of the screen. The story will be narrated automatically. You can also click the "Narrate Story" button to hear it again.
